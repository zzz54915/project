{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 인구밀도 데이터 전처리</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "excel_file = 'data/2023년도 지역별 인구밀도.xlsx'\n",
    "df = pd.read_excel(excel_file, engine='openpyxl')\n",
    "\n",
    "# 데이터프레임을 JSON 형식으로 변환\n",
    "json_data = df.to_json(orient='records', force_ascii=False)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('data/2023지역별인구밀도.json', 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 컬럼 확인:\n",
      "Index(['Unnamed', '인구', '인구밀도'], dtype='object', name=0)\n",
      "전처리된 데이터 확인:\n",
      "   지역    인구    인구밀도\n",
      "0  서울  9400  15.533\n",
      "1  부산  3284   4.258\n",
      "2  대구  2360   2.666\n",
      "3  인천  3009   2.820\n",
      "4  광주  1463   2.921\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# JSON 파일 읽기\n",
    "json_file = 'data/2023지역별인구밀도.json'\n",
    "df = pd.read_json(json_file)\n",
    "\n",
    "# 첫 번째 행을 헤더로 설정하고, 나머지 행을 데이터로 설정\n",
    "df.columns = df.iloc[0]  # 첫 번째 행을 열 이름으로 사용\n",
    "df = df.drop(0).reset_index(drop=True)  # 첫 번째 행을 데이터에서 제거\n",
    "\n",
    "# 컬럼 이름에 None이 있는 경우 처리\n",
    "df.columns = df.columns.fillna('Unnamed')\n",
    "\n",
    "# 데이터 확인: 현재 열 이름을 출력하여 확인합니다.\n",
    "print(\"데이터 컬럼 확인:\")\n",
    "print(df.columns)\n",
    "\n",
    "# 불필요한 열 제거: 'Unnamed'로 시작하는 열 이름을 사용\n",
    "df = df[['Unnamed', '인구', '인구밀도']]  # 필요한 열만 선택\n",
    "\n",
    "# 컬럼 이름 변경\n",
    "df.columns = ['지역', '인구', '인구밀도']\n",
    "\n",
    "# '계' 행 제거 (합계 관련 행)\n",
    "df = df[df['지역'] != '계'].reset_index(drop=True)\n",
    "\n",
    "# 쉼표 제거 후 숫자형으로 변환\n",
    "df['인구'] = df['인구'].apply(lambda x: int(x.replace(',', '')) if isinstance(x, str) else x)\n",
    "df['인구밀도'] = df['인구밀도'].apply(lambda x: int(x.replace(',', '')) if isinstance(x, str) else x)\n",
    "\n",
    "# 인구밀도 단위 변경 (천명/㎢로 변환)\n",
    "df['인구밀도'] = df['인구밀도'] / 1000  # 천명으로 변환\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"전처리된 데이터 확인:\")\n",
    "print(df.head())  # 전처리된 데이터 확인\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2024 전국 문화기반시설 총람 데이터 전처리</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국립도서관 탭이 data/국립도서관.json로 저장되었습니다.\n",
      "공공도서관 탭이 data/공공도서관.json로 저장되었습니다.\n",
      "박물관 탭이 data/박물관.json로 저장되었습니다.\n",
      "미술관 탭이 data/미술관.json로 저장되었습니다.\n",
      "생활문화센터 탭이 data/생활문화센터.json로 저장되었습니다.\n",
      "문예회관 탭이 data/문예회관.json로 저장되었습니다.\n",
      "지방문화원 탭이 data/지방문화원.json로 저장되었습니다.\n",
      "문화의집 탭이 data/문화의집.json로 저장되었습니다.\n",
      "문학관 탭이 data/문학관.json로 저장되었습니다.\n",
      "(부록)지역문화재단 탭이 data/(부록)지역문화재단.json로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "excel_file = 'data/2024 전국 문화기반시설 총람.xlsx'\n",
    "\n",
    "# 엑셀 파일 내의 모든 시트(탭) 읽기\n",
    "df = pd.read_excel(excel_file, sheet_name=None)\n",
    "\n",
    "# 각 시트를 JSON 파일로 저장\n",
    "for sheet_name, data in df.items():\n",
    "    # JSON 파일로 저장\n",
    "    json_file = f'data/{sheet_name}.json'\n",
    "    data.to_json(json_file, orient='records', force_ascii=False, indent=4)\n",
    "\n",
    "    print(f'{sheet_name} 탭이 {json_file}로 저장되었습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    연번  시도   시군구 설립주체      도서관명                          주소\n",
      "0  1.0  서울   서초구   국립   국립중앙도서관    서울특별시 서초구 반포대로 201 (반포동)\n",
      "1  2.0  서울  영등포구   국립     국회도서관   서울특별시 영등포구 의사당대로 1 (여의도동)\n",
      "2  3.0  서울   서초구   국립  국립장애인도서관    서울특별시 서초구 반포대로 201 (반포동)\n",
      "3  4.0  경기   고양시   국립     법원도서관  경기도 고양시 일산동구 호수로 550 (장항동)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_file = 'data/2024_전국_문화기반시설_총람.json'\n",
    "\n",
    "# JSON 데이터 로드\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 국립도서관 데이터 로드\n",
    "df_national = pd.DataFrame(data[\"국립도서관\"])\n",
    "\n",
    "# 필요한 컬럼만 선택 (시도, 시군구, 설립주체, 도서관명, 주소)\n",
    "df_national_cleaned = df_national[['연번','시도', '시군구', '설립주체', '도서관명', '주소']]\n",
    "\n",
    "# 데이터 확인\n",
    "print(df_national_cleaned.head())\n",
    "\n",
    "# JSON 파일로 저장 (orient='records'로 저장하여 각 행을 개별 JSON 객체로 저장)\n",
    "df_national_cleaned.to_json('data/국립도서관_cleaned.json', orient='records', force_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  연번  시도  시군구 설립주체             도서관명                                        주소\n",
      "0  1  서울  강남구  교육청    서울특별시교육청강남도서관  서울특별시 강남구 선릉로116길 45 (삼성동) 서울특별시교육청강남도서관\n",
      "1  2  서울  강남구  교육청    서울특별시교육청개포도서관   서울특별시 강남구 선릉로4길 30 (개포동) 서울특별시교육청 개포도서관\n",
      "2  3  서울  강동구  교육청    서울특별시교육청강동도서관          서울특별시 강동구 양재대로116길 57 (길동) 강동도서관\n",
      "3  4  서울  강동구  교육청  서울특별시교육청고덕평생학습관           서울특별시 강동구 고덕로 295 (고덕동) 고덕평생학습관\n",
      "4  5  서울  강서구  교육청    서울특별시교육청강서도서관          서울특별시 강서구 등촌로51나길 29 (등촌동) 강서도서관\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_file = 'data/2024_전국_문화기반시설_총람.json'\n",
    "\n",
    "# JSON 데이터 로드\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 공공도서관 데이터 로드\n",
    "df_public = pd.DataFrame(data[\"공공도서관\"])\n",
    "\n",
    "# 필요한 컬럼만 선택 (연번, 시도, 시군구, 설립주체, 도서관명)\n",
    "df_public_cleaned = df_public[['연번', '시도', '시군구', '설립주체', '도서관명','주소']]\n",
    "\n",
    "# 데이터 확인\n",
    "print(df_public_cleaned.head())\n",
    "\n",
    "# JSON 파일로 저장\n",
    "df_public_cleaned.to_json('data/공공도서관_cleaned.json', orient='records', force_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    연번     시도  시군구       시설명                   주    소\n",
      "0  NaN    NaN  NaN       NaN                      NaN\n",
      "1  1.0  서울특별시  강남구    강남구민회관  서울특별시 강남구 삼성로 154 (대치동)\n",
      "2  2.0  서울특별시  강동구    강동아트센터         서울특별시 강동구 동남로870\n",
      "3  3.0  서울특별시  강북구  강북문화예술회관        서울특별시 강북구 삼각산로 85\n",
      "4  4.0  서울특별시  강서구    강서구민회관        서울특별시 강서구 우장산로 66\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# JSON 파일 로드\n",
    "json_file = 'data/2024_전국_문화기반시설_총람.json'\n",
    "\n",
    "# JSON 데이터 로드\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 문예회관 데이터 로드\n",
    "df_culture = pd.DataFrame(data[\"문예회관\"])\n",
    "\n",
    "# 첫 번째 행(결측값)을 제거\n",
    "df_culture_cleaned = df_culture.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "# 필요한 컬럼만 선택 (연번, 시도, 시군구, 건립주체, 시설명, 주    소)\n",
    "df_culture_cleaned = df_culture_cleaned[['연번', '시도', '시군구', '시설명', '주    소']]\n",
    "\n",
    "# 데이터 확인\n",
    "print(df_culture_cleaned.head())\n",
    "\n",
    "# JSON 파일로 저장\n",
    "df_culture_cleaned.to_json('data/문예회관_cleaned.json', orient='records', force_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   연번  시도  시군구                          박물관명                  주소\n",
      "2   1  서울  용산구                       국립중앙박물관  서울특별시 용산구 서빙고로 137\n",
      "3   2  서울  종로구                       국립민속박물관    서울특별시 종로구 삼청로 37\n",
      "4   3  경기  파주시  국립민속박물관 파주\\n(개방형 수장고 및 정보센터)    경기도 파주시 헤이리로 30 \n",
      "5   4  서울  종로구                     대한민국역사박물관  서울특별시 종로구 세종대로 198\n",
      "6   5  서울  용산구                       국립한글박물관  서울특별시 용산구 서빙고로 139\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel 파일 경로\n",
    "file_path = 'data/2024 전국 문화기반시설 총람.xlsx'\n",
    "\n",
    "# '박물관' 시트에서 데이터 로드\n",
    "df_museum = pd.read_excel(file_path, sheet_name='박물관')\n",
    "\n",
    "# 열 이름의 공백 제거\n",
    "df_museum.columns = df_museum.columns.str.strip()\n",
    "\n",
    "# 불필요한 'Unnamed'로 시작하는 열 제거\n",
    "df_museum_cleaned = df_museum.loc[:, ~df_museum.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# 필요한 열만 선택 (연번, 시도, 시군구, 박물관명, 주소)\n",
    "df_museum_cleaned = df_museum_cleaned[['연번', '시도', '시군구', '박물관명', '주소']]\n",
    "\n",
    "# 연번 1번, 2번 삭제\n",
    "df_museum_cleaned = df_museum_cleaned.drop([0, 1])  # 첫 번째, 두 번째 행 삭제\n",
    "\n",
    "# 연번을 1부터 새로 매기기\n",
    "df_museum_cleaned['연번'] = range(1, len(df_museum_cleaned) + 1)\n",
    "\n",
    "# 데이터 확인\n",
    "print(df_museum_cleaned.head())\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df_museum_cleaned.to_json('data/박물관_cleaned_with_new_연번.json', orient='records', force_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    연번   시도  시군구                          박물관명                  주소\n",
      "0  NaN  NaN  NaN                           NaN                 NaN\n",
      "1  NaN  NaN  NaN                           NaN                 NaN\n",
      "2  1.0   서울  용산구                       국립중앙박물관  서울특별시 용산구 서빙고로 137\n",
      "3  2.0   서울  종로구                       국립민속박물관    서울특별시 종로구 삼청로 37\n",
      "4  NaN   경기  파주시  국립민속박물관 파주\\n(개방형 수장고 및 정보센터)    경기도 파주시 헤이리로 30 \n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "print(df_museum_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3940_RC01/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"영화관\",\"geo\":\"KR\",\"time\":\"today 12-m\"},{\"keyword\":\"박물관\",\"geo\":\"KR\",\"time\":\"today 12-m\"},{\"keyword\":\"미술관\",\"geo\":\"KR\",\"time\":\"today 12-m\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"geo=KR&q=%EC%98%81%ED%99%94%EA%B4%80,%EB%B0%95%EB%AC%BC%EA%B4%80,%EB%AF%B8%EC%88%A0%EA%B4%80&hl=ko&date=today 12-m,today 12-m,today 12-m\",\"guestPath\":\"https://trends.google.co.kr:443/trends/embed/\"}); </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3940_RC01/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"영화관\",\"geo\":\"KR\",\"time\":\"today 12-m\"},{\"keyword\":\"박물관\",\"geo\":\"KR\",\"time\":\"today 12-m\"},{\"keyword\":\"미술관\",\"geo\":\"KR\",\"time\":\"today 12-m\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"geo=KR&q=%EC%98%81%ED%99%94%EA%B4%80,%EB%B0%95%EB%AC%BC%EA%B4%80,%EB%AF%B8%EC%88%A0%EA%B4%80&hl=ko&date=today 12-m,today 12-m,today 12-m\",\"guestPath\":\"https://trends.google.co.kr:443/trends/embed/\"}); </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>네이버 리뷰 크롤링</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/envs/study/lib/python3.12/site-packages (4.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/envs/study/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during click 11: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".TeItc\"}\n",
      "  (Session info: chrome=131.0.6778.205); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001008e3184 cxxbridge1$str$ptr + 3626716\n",
      "1   chromedriver                        0x00000001008db9d4 cxxbridge1$str$ptr + 3596076\n",
      "2   chromedriver                        0x0000000100348968 cxxbridge1$string$len + 89228\n",
      "3   chromedriver                        0x000000010038cd50 cxxbridge1$string$len + 368756\n",
      "4   chromedriver                        0x00000001003c65b4 cxxbridge1$string$len + 604376\n",
      "5   chromedriver                        0x0000000100381568 cxxbridge1$string$len + 321676\n",
      "6   chromedriver                        0x00000001003821b8 cxxbridge1$string$len + 324828\n",
      "7   chromedriver                        0x00000001008ae9ac cxxbridge1$str$ptr + 3411716\n",
      "8   chromedriver                        0x00000001008b1ccc cxxbridge1$str$ptr + 3424804\n",
      "9   chromedriver                        0x000000010089586c cxxbridge1$str$ptr + 3308996\n",
      "10  chromedriver                        0x00000001008b258c cxxbridge1$str$ptr + 3427044\n",
      "11  chromedriver                        0x000000010088709c cxxbridge1$str$ptr + 3249652\n",
      "12  chromedriver                        0x00000001008cc4b8 cxxbridge1$str$ptr + 3533328\n",
      "13  chromedriver                        0x00000001008cc634 cxxbridge1$str$ptr + 3533708\n",
      "14  chromedriver                        0x00000001008db648 cxxbridge1$str$ptr + 3595168\n",
      "15  libsystem_pthread.dylib             0x000000018fc29034 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x000000018fc23e3c thread_start + 8\n",
      "\n",
      "Found 102 reviews\n",
      "Reviews saved to naver_reviews_2025-01-03_15-30-14.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import csv\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# url\n",
    "url = 'https://m.place.naver.com/place/11621150/review/visitor'\n",
    "\n",
    "# Webdriver headless mode setting\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080')\n",
    "options.add_argument(\"disable-gpu\")\n",
    "\n",
    "# BS4 setting for secondary access\n",
    "session = requests.Session()\n",
    "headers = {\n",
    "    \"User-Agent\": \"user value\"}\n",
    "\n",
    "retries = Retry(total=5,\n",
    "                backoff_factor=0.1,\n",
    "                status_forcelist=[500, 502, 503, 504])\n",
    "\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# New CSV file\n",
    "now = datetime.datetime.now()\n",
    "csv_file = 'naver_reviews_' + now.strftime('%Y-%m-%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "# Start crawling/scraping!\n",
    "try:\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait until the page is fully loaded and the span element with class 'TeItc' is clickable\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CLASS_NAME, 'TeItc'))\n",
    "    )\n",
    "    \n",
    "    # Click the span element with class 'TeItc' 30 times\n",
    "    for i in range(30):  # Maximum 30 clicks\n",
    "        try:\n",
    "            # Find the span element by its class name and click it\n",
    "            span_element = driver.find_element(By.CLASS_NAME, 'TeItc')\n",
    "            span_element.click()\n",
    "            time.sleep(2)  # Wait for the page to load after each click\n",
    "        except Exception as e:\n",
    "            print(f\"Error during click {i + 1}: {e}\")\n",
    "            break  # If an error occurs, stop the process\n",
    "    \n",
    "    # Wait for some time after the last click to ensure all content has loaded\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Get the page source after clicking\n",
    "    html = driver.page_source\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Extract the reviews\n",
    "    reviews = bs.select('li.pui__X35jYm.EjjAW')  # Ensure this CSS selector is correct\n",
    "\n",
    "    # If reviews are found, process them\n",
    "    if reviews:\n",
    "        print(f\"Found {len(reviews)} reviews\")\n",
    "        \n",
    "        # Write reviews to CSV file\n",
    "        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['content'])  # Writing the header\n",
    "            for r in reviews:\n",
    "                # content\n",
    "                content = r.select_one('div.pui__vn15t2 > a')  # Ensure this selector is correct\n",
    "\n",
    "                # exception handling\n",
    "                content = content.text if content else ''\n",
    "\n",
    "                # Write content to CSV\n",
    "                writer.writerow([content])\n",
    "                time.sleep(0.06)\n",
    "    else:\n",
    "        print(\"No reviews found.\")\n",
    "    \n",
    "    print(f\"Reviews saved to {csv_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # Save CSV file even if an error occurs\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['content'])  # Writing the header\n",
    "    print(f\"Reviews saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     국립중앙박물관   너무  다양한   많은   보고   있는        있다        전시  좋아요   좋은\n",
      "0        0.0  0.0  0.0  0.0  0.0  0.0  1.000000  0.000000  0.0  0.0\n",
      "1        0.0  0.0  0.0  0.0  0.0  0.0  0.716154  0.697943  0.0  0.0\n",
      "2        0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0\n",
      "3        0.0  0.0  0.0  0.0  0.0  0.0  1.000000  0.000000  0.0  0.0\n",
      "4        1.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0\n",
      "..       ...  ...  ...  ...  ...  ...       ...       ...  ...  ...\n",
      "286      0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0\n",
      "287      0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  1.0\n",
      "288      0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0\n",
      "289      0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0\n",
      "290      0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.0  0.0\n",
      "\n",
      "[291 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file = 'naver_reviews_2025-01-03_15-23-22.csv'  # 예시 파일 이름\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# 리뷰 내용이 저장된 컬럼명 확인 (예시에서는 'content'로 되어있음)\n",
    "reviews = df['content'].dropna()\n",
    "\n",
    "# TF-IDF 벡터화기\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # 'english'는 불용어 제거, max_features로 최댓값 설정\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# 결과 출력\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# 출력하여 확인\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
